{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# One between-subject factor\n\n",
      "metadata": {},
      "id": "40290eb1"
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport pingouin as pg\npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.max_columns', 20)\ndf = pg.read_dataset('mixed_anova.csv')\npg.pairwise_ttests(dv='Scores', between='Group', data=df).round(3)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ab67c8d7"
    },
    {
      "cell_type": "markdown",
      "source": "# One within-subject factor\n\n",
      "metadata": {},
      "id": "568bc7c3"
    },
    {
      "cell_type": "code",
      "source": "post_hocs = pg.pairwise_ttests(dv='Scores', within='Time', subject='Subject', data=df)\npost_hocs.round(3)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ff1133b4"
    },
    {
      "cell_type": "markdown",
      "source": "# Non-parametric pairwise paired test (wilcoxon)\n\n",
      "metadata": {},
      "id": "6dbec605"
    },
    {
      "cell_type": "code",
      "source": "pg.pairwise_ttests(dv='Scores', within='Time', subject='Subject',\n                   data=df, parametric=False).round(3)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "bd92102a"
    },
    {
      "cell_type": "markdown",
      "source": "# Mixed design (within and between) with bonferroni-corrected p-values\n\n",
      "metadata": {},
      "id": "623d8407"
    },
    {
      "cell_type": "code",
      "source": "posthocs = pg.pairwise_ttests(dv='Scores', within='Time', subject='Subject',\n                              between='Group', padjust='bonf', data=df)\nposthocs.round(3)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f79a28fa"
    },
    {
      "cell_type": "code",
      "source": "\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "1fcd4542"
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport pingouin as pg\nimport seaborn as sns\n\n# Let's assume that we have a balanced design with 30 students in each group\nn = 30\nmonths = ['August', 'January', 'June']\n\n# Generate random data\nnp.random.seed(1234)\ncontrol = np.random.normal(5.5, size=len(months) * n)\nmeditation = np.r_[np.random.normal(5.4, size=n),\n                   np.random.normal(5.8, size=n),\n                   np.random.normal(6.4, size=n)]\n\n# Create a dataframe\ndf = pd.DataFrame({'Scores': np.r_[control, meditation],\n                   'Time': np.r_[np.repeat(months, n), np.repeat(months, n)],\n                   'Group': np.repeat(['Control', 'Meditation'], len(months) * n),\n                   'Subject': np.r_[np.tile(np.arange(n), 3),\n                                    np.tile(np.arange(n, n + n), 3)]})\n# DESCRIPTIVE STATS\npg.print_table(df.head())\n\n# import seaborn as sns\nsns.set()\nsns.pointplot(data=df, x='Time', y='Scores', hue='Group', dodge=True,\n              markers=['o', 's'], capsize=.1, errwidth=1, palette='colorblind')\n\nprint(df.groupby(['Time', 'Group']).agg(['mean', 'std']))\n\n# ANOVA\naov = pg.mixed_anova(dv='Scores', within='Time', between='Group',\n                     subject='Subject', data=df)\npg.print_table(aov)\n\n# POST-HOC TESTS\nposthocs = pg.pairwise_ttests(dv='Scores', within='Time', between='Group',\n                              subject='Subject', data=df)\npg.print_table(posthocs)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9c0f139a"
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport pingouin as pg\nfrom statsmodels.stats.weightstats import ttost_paired\nfrom scipy import stats",
      "metadata": {},
      "execution_count": 3,
      "outputs": [],
      "id": "3559c9b5"
    },
    {
      "cell_type": "code",
      "source": "data = pd.read_csv('../input/ctr-a-b/ctr_a_b.csv')\ndata",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "bf0e14af"
    },
    {
      "cell_type": "markdown",
      "source": "The above table shows the A/B test results, consist of 4 columns—\"userid\" that differentiates one user from another, \"dt\" that indicates the date of experiment occured, \"groupid\" consists of 0 for Design A and 1 for Design B, and \"ctr\" that indicates the metrics result—and 447,602 rows.",
      "metadata": {},
      "id": "3807f7c7"
    },
    {
      "cell_type": "markdown",
      "source": "We can see from the above dataset info, there's one null value in ctr column. We can just drop the null column since it's only one null value. We can also change the groupid column name into design and change the value to a and b, instead of 0 and 1, to make it easier for us to understand the data.\n\n",
      "metadata": {},
      "id": "d339b63c"
    },
    {
      "cell_type": "code",
      "source": "data['ctr'] = data['ctr'].dropna()\ndata['dt'] = pd.to_datetime(data['dt'])\ndata['groupid'] = data['groupid'].replace([0, 1], ['a', 'b'])\ndata = data.rename(columns = {'groupid':'design'})\n\ndata.info()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "00e4975a"
    },
    {
      "cell_type": "code",
      "source": "data['dt'].unique()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ccedfd03"
    },
    {
      "cell_type": "code",
      "source": "data['design'].unique()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "25f43ca7"
    },
    {
      "cell_type": "code",
      "source": "len(data['userid'].unique())\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "217b87ba"
    },
    {
      "cell_type": "markdown",
      "source": "In \"dt\" column there are only 10 unique values—indicate every dates of experiment, while there are only 2 unique values in \"groupid\" colum. We can also see that the total user that get to experience the test are 59,984 users. To run the analysis, let's group the dataset by the date and the group id with the average ctr as the values.\n\n",
      "metadata": {},
      "id": "9334b68a"
    },
    {
      "cell_type": "code",
      "source": "group = data.groupby(['dt','design']).mean('ctr')\ngroup",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "14dc0d8d"
    },
    {
      "cell_type": "markdown",
      "source": "## T-Test\nA t-test is a statistical test that is used to compare the means of two groups. It is often used in hypothesis testing to determine whether a process or treatment actually has an effect on the population of interest, or whether two groups are different from one another.\n\nThe t-test is a parametric test of difference, meaning that it makes the same assumptions about your data as other parametric tests. The t-test assumes your data:\n\nare independent\nare (approximately) normally distributed.\nhave a similar amount of variance within each group being compared (a.k.a. homogeneity of variance)\nWhen choosing a t-test, you will need to consider two things: whether the groups being compared come from a single population or two different populations, and whether you want to test the difference in a specific direction.\n\n## One-sample, two-sample, or paired t-test?\n\nIf the groups come from a single population (e.g. measuring before and after an experimental treatment), perform a paired t-test.\n\nIf the groups come from two different populations (e.g. two different species, or people from two separate cities), perform a two-sample t-test (a.k.a. independent t-test).\n\nIf there is one group being compared against a standard value (e.g. comparing the acidity of a liquid to a neutral pH of 7), perform a one-sample t-test.\n\nOne-tailed or two-tailed t-test?\n\nIf you only care whether the two populations are different from one another, perform a two-tailed t-test.\n\nIf you want to know whether one population mean is greater than or less than the other, perform a one-tailed t-test.\n\nSince we are interested in the difference between two variables (Design A and Design B) for the same subject (CTR), so we'll perform paired t-test. There are 2 different methods to run paired t-test in Python. We can use SciPy and Pingouin. But first, we have to subset the data into two groups by the design in order to run the t-test.",
      "metadata": {},
      "id": "c14b4090"
    },
    {
      "cell_type": "code",
      "source": "sci = stats.ttest_rel(design_a, design_b)\nsci",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e59838aa"
    },
    {
      "cell_type": "markdown",
      "source": "## Pingouin T-Test\nBefore running the Pingouin t-test, we have run Levene's statistic test to determine the variances homogeneity of the subsets to determine the right correction to choose.",
      "metadata": {},
      "id": "3e229d74"
    },
    {
      "cell_type": "code",
      "source": "stats.levene(design_a, design_b)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f0a5b09e"
    },
    {
      "cell_type": "markdown",
      "source": "The above results show the statistic value and the p-value of Levene's test. The statistic test is 2.46 while the p-value is 0.13. Since the p-value is higher than the threshold (0.05), we can safely say that the variance in ctr between the two designs is not significantly different.\n\n",
      "metadata": {},
      "id": "0f67efeb"
    },
    {
      "cell_type": "code",
      "source": "ping = pg.ttest(design_a, design_b, paired = True, correction = False)\nping",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "25d27218"
    },
    {
      "cell_type": "markdown",
      "source": "## Analysis of T-Test for A/B Testing\nWe have generated the t-test results from two different t-test methods and both of them giving the same results in t-score, which is -895.20, and also the same results in p-value, which is 1.38 x 10^-23. The p-value is way less than the threshold, this means that we can reject the null hypothesis.\n\nRemember our hypothesis:\n\nH0 : The average CTR of Design B is less than or equal to the average CTR of Design A, μB ≤ μA\n\nHa : The average CTR of Design B is more than the average CTR of Design A, μB > μA\n\nSince we can reject the null hypothesis, means the alternative hypothesis is true. So, our marketing manager is right, design B can increase the CTR.\n\nTo support the p-value hypothesis, we can see the Cohen's d variable from Pingouin table. Cohen's d is one of the measurement to calculate the effect size. Effect size tells you how meaningful the relationship between variables or the difference between groups is. It indicates the practical significance of a research outcome.\n\nCohen’s d is designed for comparing two groups. It takes the difference between two means and expresses it in standard deviation units. It tells you how many standard deviations lie between the two means.\n\nEffect sizes can be categorized into small, medium, or large according to Cohen’s criteria. Cohen’s criteria for small, medium, and large effects differ based on the effect size measurement used. Cohen’s d can take on any number between 0 and infinity. In general, the greater the Cohen’s d, the larger the effect size.\n\nWe have 434.07 for the value of Cohen's d criteria. This means that there is a practical significance successful in changing the design from Design A to Design B.",
      "metadata": {},
      "id": "6a7cd6c6"
    },
    {
      "cell_type": "markdown",
      "source": "## Conclusion\nWe have run an A/B test to see the average/mean of the CTR for two different designs. From the t-test, we get p-value lower than the threshold, large Cohen's d** criteria, and high Bayes Factor, so the team can change the ad pop-up layout design from design A to design B to engage more people in clicking the ad.",
      "metadata": {},
      "id": "db471695"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "830443ed"
    }
  ]
}